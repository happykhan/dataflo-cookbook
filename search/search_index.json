{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to The Data-flo Cookbook","text":"<p>This 'cookbook' has a collection of examples or 'recipes' to show you how to make the most of Data-flo. Data-flo (pronounced data-flow) is an open-source web application for data integration. It provides an easy-to-use visual interface to design reusable Workflows (data pipelines) that import, merge, clean, and manipulate data in many different ways. Once a Workflow has been created, it can be run anytime, by anyone with access, to enable push-button data extraction and transformation.</p> <ul> <li>All examples here use the publicly available version of Data-flo at next.data-flo.io</li> <li>For detailed description of specific features in Data-flo, please read the manual</li> <li>If you have feedback and questions about Data-flo, please see the contact instructions here</li> </ul> <p>Tip</p> <p>Data-flo can be installed behind your firewall to enable you to visualise sensitive data safely. To request a quote for the supported, Docker-packaged instance, please email data-flo(at)cgps.group.</p>"},{"location":"#license","title":"License","text":"<p>Unless otherwise stated, content presented here is under a CC BY-SA 4.0 licence, which basically means you are free to:</p> <ul> <li>Share \u2014 copy and redistribute the material in any medium or format for any purpose, even commercially.</li> <li>Adapt \u2014 remix, transform, and build upon the material for any purpose, even commercially.</li> </ul> <p>Given:</p> <ul> <li>Attribution \u2014 You must give appropriate credit, provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use.</li> <li>ShareAlike \u2014 If you remix, transform, or build upon the material, you must distribute your contributions under the same license as the original.</li> </ul>"},{"location":"#how-to-use-this-cookbook","title":"How to use this cookbook","text":"<p>The best way to use this cookbook is to browse the various recipes listed on the right, and try to recreate them yourself with the example data provided. These will hopefully provide inspiration for possible visualisations you can create with your own data. You can save these recipes as a pdf, by using your browser's print function and choosing Save as pdf as the printer. </p> <p>If you have never used Data-flo before, please start with the recipe, Merging two datatables (Excel sheets). This recipe was written as an introduction to data-flo in general, as such, it will be a detailed step-by-step walkthrough. </p> <p>Tip</p> <p>If you are interested in Microreact, there is similar cookbook for Microreact at https://microreact-cookbook.netlify.app/</p>"},{"location":"about-dataflo/","title":"About Data-flo","text":"<p>Data-flo (pronounced data-flow) is an open-source web application for data integration. It provides an easy-to-use visual interface to design reusable Workflows (data pipelines) that import, merge, clean, and manipulate data in many different ways. Once a Workflow has been created, it can be run anytime, by anyone with access, to enable push-button data extraction and transformation.</p> <p>Here is a diagram of a Data-flo workflow indicating the different data sources and output destinations. </p> <p></p>"},{"location":"about-dataflo/#why-use-data-flo","title":"Why use Data-flo?","text":"<p>Data-flo saves you time by removing the bulk of the manual repetitive workflows that require multiple, sequential, or tedious steps, enabling you to focus on analysis and interpretation.</p> <p>With Data-flo, users can:</p> <ul> <li>Rapidly prepare data for visualization, and reporting </li> <li>Easily share processed data between teams </li> <li>Consistently reproduce and validate data transformation procedures for updated or new datasets </li> <li>Seamlessly integrate data from multiple databases and sources</li> <li>Automatically update a Microreact project with fresh data</li> </ul> <p>While Data-flo is used across many sectors (data science, academia, public health institutions etc.), it contains a number of features tailored to manage bioinformatics-related datasets such as Newick files, tree files, etc. </p>"},{"location":"about-dataflo/#citation","title":"Citation","text":"<p>If you use Data-flo within a publication please cite:</p> <pre><code>Centre for Genomic Pathogen Surveillance. 2019. Data-flo. https://data-flo.io. [Date accessed].\n</code></pre>"},{"location":"about-dataflo/#major-update-in-april-2024","title":"Major Update in April 2024","text":"<p>Data-flo is continuously evolving as we learn from our community and adapt its features to accommodate new use cases.</p> <p>In April 2024, a new version of Data-flo will be availble for testing on https://next.data-flo.io/, the large-scale upgrade includes significant changes to the user interface, improved ability to process large datasets, additional adaptors, and enhanced sharing permissions. This cookbook is based on the version available at https://next.data-flo.io/. </p> <p>Comparison of software interface evolution from Data-flo version 1 to the latest version of Data-flo, The image calls out changes to several user interface upgrades, new adaptors, and improved user experience. The diagram highlighting that beta-testing and client feedback have contributed to the updates made.</p> <p></p> <p>Pre-release Phase: from 1 April 2024 to 30 June 2024</p> <ul> <li>New version is made available for testing on https://next.data-flo.io/</li> <li>The current version remains unchanged and available on https://data-flo.io/. </li> <li>A banner is added to https://data-flo.io/ to invite users to try the new version.</li> </ul> <p>Release Phase: from 1 July 2024 to 31 March 2025</p> <ul> <li>The new version replaces the current version.</li> <li>Workflows on the old version are migrated to the new version. Workflows created on https://next.data-flo.io/ will continue to be available on https://data-flo.io/.</li> <li>The old version will be available to user on https://legacy.data-flo.io/ until 31 march 2025.</li> </ul> <p>Sunset Phase: from 1 April 2025 onwards</p> <ul> <li>The old version (https://legacy.data-flo.io/) will be turned off.</li> </ul>"},{"location":"about-dataflo/#interested-in-an-on-premise-installation","title":"Interested in an on-premise installation?","text":"<p>Any data submitted through Data-flo.io is processed on the Data-flo server according to the Privacy and Terms of Service. If this does not meet agency standards for protected information, Data-flo can be installed on-premise and locally administered. This local (on-premise) installation behind your firewall enables you to process sensitive data safely. With an on-premise installation, all data is retained locally and nothing is sent to the public Data-flo server. </p> <p>An on-premise Data-flo server allows you to build workflows that connect directly to internal systems, such as Laboratory Information Management Systems (LIMS), disease surveillance systems, files stored in local shared drives, and many other systems. This allows you to create secure and comprehensive data processing and cleaning workflows that can leverage both public and private data sources.</p> <p>To request a quote for the supported, Docker-packaged instance, please email us at data-flo@cgps.group</p>"},{"location":"geocoding-market/","title":"Converting addresses to latitude and longitude.","text":"<p>Prepared by Nabil-Fareed Alikhan. Last updated 2024-04-09</p> <p>Converting geographic location information, like an address into a latitude and longitude is a common hurdle for displaying data points on a map. This also affects tools like Microreact. Data-flo includes a geocoding adaptor that  can solve this problem at many levels of granuality. To illustrate this, this recipe uses a dataset that has the street address of farmers markets in the United States. We will aim to filter the data and present some of these on a map.</p>"},{"location":"geocoding-market/#data-flo-features-demonstrated","title":"Data-flo features demonstrated","text":"<ul> <li>Merging columns with <code>concatenate-columns</code></li> <li>Filtering rows with <code>filter-rows</code></li> <li>Geocoding, converting addresses into latitude and longitude with <code>geocoding</code> </li> </ul>"},{"location":"geocoding-market/#about-this-dataset","title":"About this dataset","text":"<p>The dataset used here is not related to genomics, rather to Farmers Markets in the United States. These data are available as a dataset on Kaggle.  Farmers markets are an important way for citizens to eat healthy fresh food while supporting small family farms. This data contains information about all the registered farmers markets in the United States in 2020. This includes the city, county, and information about the types of food sold at each market. </p> <p>You will need the following data: </p> <ul> <li>Data table</li> </ul>"},{"location":"geocoding-market/#filtering-and-geocoding-street-addresses","title":"Filtering and geocoding street addresses","text":"<p>This can be acheived by using the following adaptors:</p> <ul> <li><code>import-from-csv-file</code> </li> <li><code>concatenate-columns</code></li> <li><code>filter-rows</code></li> <li><code>geocoding</code> </li> <li><code>export-to-csv-file</code> </li> </ul> <p>The workflow looks like this: </p> <p></p> <p>My version of this workflow is available here. You are more than welcome to use this workflow for other data, or clone and alter it for your own needs.</p> <p>To simplify the dataset, we will focus showing farmers markets in one US state, Vermont. This is done using the <code>filter-rows</code> adaptor with the settings below. </p> <p></p> <p>To create a full address line in a single column, we use the <code>concatenate-columns</code> adaptor, taking the seperate columns like \"Street\", \"City\", \"County\" and converting that into a single column delimted time commas. That is: </p> <pre><code>Street  City    County\n153 Rte 5   Hartland    Windsor\n</code></pre> <p>Is merged into:</p> <pre><code>Location\n153 Rte 5,Hartland,Windsor\n</code></pre> <p></p> <p>To make sure each market location has the same granuality, we add a filtering step with the <code>filter rows</code> adaptor to remove any rows that does not have a Street address.</p> <p></p> <p>Flexible geocoding</p> <p>While this example uses a street addresses, the geocoding will work with less information, like with only city or country. The geocoding can also work with postcodes (e.g. CB22 3DQ) and landmarks (e.g. Big Ben)</p> <p>The filtered table is passed to a <code>geocoding</code> adaptor for the actual geocoding. We previous created the \"Location\" field in the datatable, so we can set this as the location column for geocoding.</p> <p></p> <p>The output of this step is sent to a <code>export-to-csv-file</code> adaptor. This allows you to customise the final outfile, for instance, to set the output file name to be \"geocoded_table.csv\". The resulting file can be uploaded to Microreact for visualisation. The final map from this example is shown below. If you are unsure how to upload data to Microreact, please see the guide here. </p> <p></p>"},{"location":"login-workflow/","title":"Login and create a new workflow","text":"<p>Prepared by Nabil-Fareed Alikhan. Last updated 2024-04-08</p> <p>This recipe describes the basic process of logging in and creating a new workflow. Data-flo saves you time by removing the bulk of the manual repetitive workflows that require multiple, sequential, or tedious steps, enabling you to focus on analysis and interpretation. Armed with Data-flo, users can:</p> <ul> <li>Rapidly prepare data for visualization, and reporting </li> <li>Easily share processed data between teams </li> <li>Consistently reproduce and validate data transformation procedures for updated or new datasets </li> <li>Seamlessly integrate data from multiple databases and sources</li> <li>Automatically update a Microreact project with fresh data</li> </ul> <p>While Data-flo is used across many sectors (data science, academia, public health institutions etc.), it contains a number of features tailored to manage bioinformatics-related datasets such as Newick files, tree files, etc. </p>"},{"location":"login-workflow/#data-flo-features-demonstrated","title":"Data-flo features demonstrated","text":"<ul> <li>Login and creating a new workflow</li> </ul>"},{"location":"login-workflow/#logging-in","title":"Logging in","text":"<p>Nagivate to the latest version of data-flo at https://next.data-flo.io/. You will be presented with a Splash page similar to the one below. </p> <p></p> <p>Create an account by clicking the \"Sign In\" or \"My Account\" button. You can sign in either using a passwordless option where a sign-in link is sent to your email, or through one of the account options (Google, Apple, Microsoft, Twitter/X). If you use your email, be sure to check any SPAM or junk mail folders, and add data-flo.io to your safe senders list.</p> <p>Tip</p> <p>Regardless of how you choose to login, we do not see or store your password.</p> <p></p> <p>If you've managed to log in sucessfully, you will be brought to the \"My workflows\" page. If this is your first time using data-flo, it will be blank as shown below. </p> <p></p> <p>Note</p> <p>See the Data-flo manual for more information about creating an account</p>"},{"location":"login-workflow/#creating-a-new-workflow","title":"Creating a new workflow","text":"<p>Workflows are combinations of adaptors that work together to edit, clean, merge and transform your data. To create a new workflow, click on the \"Workflows\" link in the top right. </p> <p></p> <p>This new workflow view will show workflows you have created and other workflows that have been shared with you. The button to create a new workflow is the purple plus in the bottom right. When you click the plus you will have a choice to import or create a new workflow. Click \"Create new workflow\". </p> <p></p> <p>You should now have a blank workflow canvas, looking similar to the one below.</p> <p></p> <p>Note</p> <p>See the Data-flo manual for more information about creating a workflow</p>"},{"location":"merging-two-sheets/","title":"Merging two datatables (Excel sheets)","text":"<p>Prepared by Nabil-Fareed Alikhan. Last updated 2024-04-08</p> <p>Using Data-flo, you can make many manipulations of datatables, including merging two seperate input sources together. In this recipe, we will explictly show how to join two Excel sheets using a shared column, but this approach applies to any datatable that can be loaded into Data-flo. This recipe is serving a dual purpose of being an introduction to Data-flo in general, as such, it will be a detailed step-by-step walkthrough. </p>"},{"location":"merging-two-sheets/#data-flo-features-demonstrated","title":"Data-flo features demonstrated","text":"<ul> <li>Creating a workflow </li> <li>Using File and Text inputs</li> <li>Connecting adaptors</li> <li>Customising descriptions and names</li> <li>Creating output</li> <li>Testing work flows</li> <li><code>join-datatables</code> adaptor</li> <li><code>export-to-csv-file</code> adaptor</li> <li><code>import-from-excel-file</code> adaptor</li> </ul>"},{"location":"merging-two-sheets/#about-this-dataset","title":"About this dataset","text":"<p>To illustrate these aspects of Microreact, we will use the dataset described in Alikhan et al. (2022)<sup>1</sup>. This study look at non-typhoidal Salmonella enterica, which can be from contaminated poultry meat, and causes diarrhoeal disease. This study examines 183 Brazilian chicken and 357 UK-imported poultry product genomes. </p> <p>In this publication, the contextual metadata, such as sample collection location and time, and the genome assembly metrics are split across two different Excel files. We want to merge these two files into a single table. </p> <p>For this exercise you will require the following files:</p> <ul> <li>Link to sample metadata table</li> <li>Link to genome assembly metrics table</li> </ul>"},{"location":"merging-two-sheets/#merging-the-two-files-together-with-an-existing-workflow","title":"Merging the two files together with an existing workflow","text":"<p>Start by downloading the two Excel files from the links above. If we inspect both files (using Excel), we will see that the column \"ID\" is the column with the sample ID and is shared between the two.</p> <p>You can use my existing workflow available here. You will need to be logged in to use it.</p> <p></p> <p>To use it, click on each of the input file fields and use to file browser window to upload the first file (sample-in-this-study.xlsx) to Excel 1, and the second file (assembly-metrics.xlsx) to Excel 2. We know that the \"ID\" column is the one shared between the two files, so enter this as the input for column name. </p> <p>Warning</p> <p>In this workflow, the column name is case-sensitive. So the input must be \"ID\" not \"id\" or \"Id\". </p> <p>Your input window should look something like this: </p> <p></p> <p>If this is the case, then click \"RUN\". It will take a few seconds for the workflow to run, if you are successful you will see contents of the resulting comma delimited file (csv). You can download the file with the download link on the right. </p> <p></p> <p>If you were unsuccessful, you will see an empty output. </p> <p></p> <p>In either case, you can re-run the workflow by clicking the RUN AGAIN button. </p> <p>Tip</p> <p>You are more than welcome to use this workflow for other data, or clone and alter it for your own needs. </p> <p>This is a very simple manipulation shown here in isolation. This step can be one of many other steps in a single workflow. For simplicity in this example, we have manually uploaded the input files but there are many possible input sources in Data-flo. These include (but are not limited to) other file formats like JSON or csv; online locations like Google sheets, Google drive, and Dropbox; and direct connections to databases.</p>"},{"location":"merging-two-sheets/#creating-the-workflow","title":"Creating the workflow","text":"<p>If the existing workflow above meets your needs, then by all means clone it and keep a copy for yourself. However, you may be interested in making your own workflow specific to your use-case. It is here where I will outline the steps taken to make the workflow above. You should be able to create a new blank workflow for this recipe. If you are unsure how to do that please complete prerequisite recipe: Login and create a new workflow. We are aiming to make a workflow like the one below. </p> <p></p> <p>Starting with a new and blank workflow, add the <code>import-from-excel-file</code> adaptor. If you are unsure how to do this you can clear the add step button in the top left menu, or clear the add new step link on the right, as show below. </p> <p></p> <p></p> <p>Note</p> <p>See the Data-flo manual for more information about the interface</p> <p>From the adaptor menu, which shows up on the right be default, look for the <code>import-from-excel-file</code> adaptor which is under the \"Import\" subheading. The search bar can also help filter through the adaptors, use search terms such as \"excel\" or \"import\". Either you should be able to find the <code>import-from-excel-file</code> adaptor in the list. </p> <p></p> <p>Now click on it to add it to the workflow canvas. By clicking on the header of the adaptor, the adaptor search will be replaced with a panel detailing the settings for the adaptor, as shown below. </p> <p></p> <p>Open the section for \"file\" input, and set the input to \"data-flo input\" and in the dataflo input dropdown, select \"Create new input\". It should look something like the image below. </p> <p></p> <p>There will be a new item on the canvas added, called Input-1. We can customise it by clicking on it and changing the settings. </p> <p></p> <p>Try changing the settings. Change the input name and the description. These values will be reflected in the form presented to a user, i.e. the thing we interacted with in the previous section. You can also upload test data to help you test the workflow while you are working on it. Use our example data, \"sample-in-this-study.xlsx\". </p> <p></p> <p>We now have an input step for the first Excel file and we need to repeat the entire process of adding and configuring an <code>import-from-excel-file</code> adaptor for the second file. Repeat the previous few steps and create another <code>import-from-excel-file</code> adaptor. Be sure to have different dataflo inputs for each <code>import-from-excel-file</code> adaptor. You canvas should look like the one below. </p> <p>Note</p> <p>See the Data-flo manual for more information about adaptors</p> <p></p> <p>Let's add an adaptor to merge the two tables, this is called <code>join-datatables</code>. Locate and add this to the canvas.</p> <p></p> <p>For the \"main column\" setting, set the input to be another new input. This will be the field where a user can enter the column name on which to join the two columns. You can configure the input as we did for the others. </p> <p></p> <p></p> <p>If you haven't already, join the <code>join-datatables</code> to the <code>import-from-excel-file</code> adaptors. To do this, click on the output items (right hand side items) of an adaptor and you will see a number of inputs  (left hand side) for other adaptors highlighted with a (+) plus. By clicking on the plus, you can connect the two adaptors from one to the other. If you make a mistake and want to break a connection, mouse over an existing connection (the middle of the line/arrow) and an X will appear. Clicking on this will break the connection. </p> <p></p> <p>The canvas should now look like this, with three inputs, two <code>import-from-excel-file</code> adaptors, connecting into one <code>join-datatables</code> adaptor. </p> <p></p> <p>If you have added the sample data as test data into the wokrflow, you can press the \"run\" button (play icon) in the top left menu and see the output in the \"data\" output item of the <code>join-datatables</code> adaptor.  </p> <p></p> <p>The last stage is to add an adaptor to create an output file. We will use the <code>export-to-csv-file</code> adaptor, which can be added in the same way as the others. </p> <p></p> <p>For the <code>export-to-csv-file</code> adaptor, set the file output to \"Use as workflow output\" as shown below. This will create an output item, which you can customise just like the input ones. Connet the <code>export-to-csv-file</code> adaptor to the <code>join-datatables</code> adaptor. </p> <p></p> <p></p> <p>The final results will appear under \"file\" of the <code>export-to-csv-file</code> adaptor. </p> <p></p> <p>And that's it! By working through this recipe you have covered the basic functionality of data-flo. The workflow has been automatically saving as we have been working but you can explictly save it with the save icon in the top left menu, if it isn't greyed out. </p> <p>Clicking on the \"Run\" option in the top right, will take you to the interactive form similar to what we used in the previous section. If you would like to share this workflow, the share button in the top right will allow you to customise the sharing options to your liking and give you a link that you can share with your colleagues. </p> <ol> <li> <p>Alikhan NF, Moreno LZ, Castellanos LR, Chattaway MA, McLauchlin J, et al. (2022) Dynamics of Salmonella enterica and antimicrobial resistance in the Brazilian poultry industry and global impacts on public health. PLOS Genetics 18(6): e1010174. https://doi.org/10.1371/journal.pgen.1010174\u00a0\u21a9</p> </li> </ol>"},{"location":"removing-duplicates/","title":"Removing duplicates from a dataset","text":"<p>Prepared by Nabil-Fareed Alikhan. Last updated 2024-04-08</p> <p>This recipe will show how to use the <code>remove-duplicate-rows</code> adaptor to filter duplicate rows in a datatable. </p>"},{"location":"removing-duplicates/#data-flo-features-demonstrated","title":"Data-flo features demonstrated","text":"<ul> <li><code>import-from-csv-file</code> adaptor</li> <li><code>remove-duplicate-rows</code> adaptor</li> <li>Interactive table filtering options</li> </ul>"},{"location":"removing-duplicates/#about-this-dataset","title":"About this dataset","text":"<p>The data used here was originally presented in Wong et al (2015)<sup>1</sup>. The study is about the genome sequencing of 1,832 Salmonella enterica ser. Typhi, which revealed a dominant MDR lineage, H58, spreading across Asia and Africa over three decades. H58 displaces sensitive strains, with complex MDR elements on plasmids or chromosomes. </p> <p>I have taken their sample data sheet, which had no errors, and manually added in some duplicates for the sake of this recipe. The modified file is available below:</p> <ul> <li>Link to data table</li> </ul>"},{"location":"removing-duplicates/#removing-duplicates","title":"Removing duplicates","text":"<p>You should be able to create a new blank workflow for this recipe. If you are unsure how to do that please complete prerequisite recipe: Login and create a new workflow. Adding and connecting adaptors was explained in detail in the merging two Excel sheets recipe. If you are comfortable with the content presented in both examples, then let's continue. </p> <p></p> <p>A workflow for removing duplicate rows can be done by using an Import adaptor, such as <code>import-from-csv-file</code> adaptor connected to the <code>remove-duplicate-rows</code> adaptor. A full description of the <code>remove-duplicate-rows</code> adaptor can be found here. By default the <code>remove-duplicate-rows</code> adaptor will use the entire row for comparison when looking for duplicate. You can specify a specific column (or columns) to be used under the column setting. In this case, the column \"id\" holds the sample Id, which should be unique, and is a good option in this case. The <code>remove-duplicate-rows</code> adaptor has two outputs, one of the deduplicated rows and another of the duplicated rows. You can use redirect the duplicated rows else, or use it as an output for logging purposes. By the way, the <code>remove-duplicate-rows</code> adaptor will retain the first instance out of the duplicate rows.</p> <p></p> <p>Try upload the data table to the \"test run value\" found under the \"Input\" item, and then performing a test run (by using the play button in the top left menu). This will allow you to see the outcome of this adaptor while working on the workflow. You can see the specific output by clicking on either \"data\" or \"duplicates\". </p>"},{"location":"removing-duplicates/#an-interactive-approach","title":"An interactive approach","text":"<p>There is an interactive approach which allows you to review results and quickly chain adaptors for your workflow. Try upload the data table to the \"test run value\" found under the \"Input\" item, and then performing a test run (by using the play button in the top left menu). This will allow you to see the outcome of this adaptor while working on the workflow. You can see the specific output by clicking on either \"data\" or \"duplicates\". </p> <p>By clicking the down arrow near the column name for any of the output previews, you can choose an step to apply to the data table.</p> <p></p> <p>This will automatically create a new adaptor, such as <code>remove-duplicate-rows</code>, to perform the task. It will prepopulate the adaptor with some information. In this case, from the output of the <code>import-from-csv-file</code> on the Id column, I selected the option to remove duplicates on that column, so data-flo automatically added and attached a <code>remove-duplicate-rows</code> with the \"id\" column prefilled as the column name on which to check for duplicates. </p> <p></p> <p>This can be done as many times as you like, by the following set of steps: </p> <ul> <li>Run the \"test run\"</li> <li>Look at the output from next output item </li> <li>Choose a process you would like to apply </li> <li>Repeat (i.e re-run \"test run\"...)</li> </ul> <p>Try chaining multiple steps using this process to create a multi-step workflow like below.</p> <p></p> <p>For any of the output items you could like to save, check the option \"Use as Workflow Output\" to have the output presented when running the workflow. </p> <p>Tip</p> <p>For more information about the removing duplicate rows, see the manual</p> <ol> <li> <p>Wong VK, Baker S, Pickard DJ, Parkhill J, Page AJ, Feasey NA, Kingsley RA, Thomson NR, Keane JA, Weill FX, Edwards DJ, Hawkey J, Harris SR, Mather AE, Cain AK, Hadfield J, Hart PJ, Thieu NT, Klemm EJ, Glinos DA, Breiman RF, Watson CH, Kariuki S, Gordon MA, Heyderman RS, Okoro C, Jacobs J, Lunguya O, Edmunds WJ, Msefula C, Chabalgoity JA, Kama M, Jenkins K, Dutta S, Marks F, Campos J, Thompson C, Obaro S, MacLennan CA, Dolecek C, Keddy KH, Smith AM, Parry CM, Karkey A, Mulholland EK, Campbell JI, Dongol S, Basnyat B, Dufour M, Bandaranayake D, Naseri TT, Singh SP, Hatta M, Newton P, Onsare RS, Isaia L, Dance D, Davong V, Thwaites G, Wijedoru L, Crump JA, De Pinna E, Nair S, Nilles EJ, Thanh DP, Turner P, Soeng S, Valcanis M, Powling J, Dimovski K, Hogg G, Farrar J, Holt KE, Dougan G. Phylogeographical analysis of the dominant multidrug-resistant H58 clade of Salmonella Typhi identifies inter- and intracontinental transmission events. Nat Genet. 2015 Jun;47(6):632-9. doi: 10.1038/ng.3281. Epub 2015 May 11. PMID: 25961941; PMCID: PMC4921243.\u00a0\u21a9</p> </li> </ol>"},{"location":"renaming-newick-files/","title":"Renaming labels in a Newick tree file","text":"<p>Prepared by Nabil-Fareed Alikhan. Last updated 2024-04-09</p> <p>A Newick tree is a common way to represent phlogenetic trees. Newick files can be processed by Data-flo and be tranformed in the same way as many other data. In this specific recipe we will show a solution to a common problem of  renaming the tip labels of a tree. The intial tree was generated while we were using has an internal ID (<code>SAL_</code>) from Enterobase, but we want to change these labels to the SRA accession, which is the final accession code from the  public databases where we have deposited our sequencing data. You can see the initial tree below with the <code>SAL_</code> as labels. </p> <p></p> <p>The Newick data format is text but can be difficult to understand and parse, as shown below. Data-flo has a specific adaptor, <code>rename-newick-leaf-labels</code> that can help.</p> <pre><code>((SAL_IA9015AA_AS:0.00016804,((SAL_EB1343AA_AS:9.3034e-06,SAL_CB2252AA_AS:1.13873e-05)1:2.90208e-07,\n((SAL_KA8766AA_AS:4.43003e-06,(SAL_AB9743AA_AS:5.53755e-06,SAL_DB6670AA_AS:6.64654e-07)1:8.8599e-07)\n1:1.77201e-06,(((SAL_DB4383AA_AS:1.32901e-06,(SAL_EA6989AA_AS:1.9935e-06,\n(SAL_KA0650AA_AS:1.55053e-06,SAL_CB7382AA_AS:1.99349e-06)1:6.64491e-07)1:6.64487e-07)1:2.658e-06,\n(SAL_BA3495AA_AS:1.55066e-06,(((SAL_DB3912AA_AS:3.54407e-06,SAL_IA9934AA_AS:7.53117e-06)1:4.42923e-07,\n(SAL_DB3929AA_AS:7.53108e-06,((((SAL_DB3917AA_AS:1e-09, ...\n</code></pre>"},{"location":"renaming-newick-files/#data-flo-features-demonstrated","title":"Data-flo features demonstrated","text":"<ul> <li><code>rename-newick-leaf-labels</code> adaptor.</li> </ul> <p>What is a Newick file?</p> <p>A Newick file is a text-based format used to represent phylogenetic trees, which depict evolutionary relationships between biological entities such as species, genes, or sequences. In a Newick file, the tree is represented as a nested set of parentheses and commas, with each node (representing a taxonomic group or ancestral lineage) labeled with its corresponding name or identifier. The format allows for the representation of hierarchical relationships and branch lengths, making it widely used in bioinformatics and evolutionary biology for storing and exchanging tree data.</p>"},{"location":"renaming-newick-files/#about-this-dataset","title":"About this dataset","text":"<p>This dataset is from Zhou et al (2018)<sup>1</sup>. The particular study mentioned in the paper looked at Salmonella enterica ser. Agama in badgers (2006\u20132007, Woodchester Park, England). We sequenced 72 isolates, analyzing population structure in a single host species over a small area and comparing genomes across hosts and locations. The tree used here is a preliminary tree that has an old set of IDs we wish to change. The mapping of old label to new label is available in the mapping table file below. </p> <p>For this recipe you will need the following: </p> <ul> <li>Original tree file (newick)</li> <li>Mapping table</li> </ul>"},{"location":"renaming-newick-files/#renaming-tip-labels","title":"Renaming tip labels","text":"<p>This can be acheived by using the following adaptors:</p> <ul> <li><code>rename-newick-leaf-labels</code></li> <li><code>import-text-from-file</code></li> <li><code>import-from-csv-file</code> </li> <li><code>create-dictionary-from-datatable</code></li> <li><code>rename-newick-leaf-labels</code></li> <li><code>export-text-to-file</code> </li> </ul> <p>The workflow looks like this: </p> <p></p> <p>My version of this workflow is available here. You are more than welcome to use this workflow for other data, or clone and alter it for your own needs. </p> <p>I started with the <code>rename-newick-leaf-labels</code> adaptor and worked backwards to make sure it had the right input. The \"newick\" input <code>rename-newick-leaf-labels</code> requires a Newick file, which is ultimately a text file, so to have an import route, I used the <code>import-text-from-file</code> adaptor. The \"mapping\" input <code>rename-newick-leaf-labels</code> requires a dictionary. To create the dictionary, I used the <code>create-dictionary-from-datatable</code> from datatable adaptor; specifying which columns in the input mapping table would be the key and the value, Label and Rename respectively. These values are hard-coded using the \"Define value\" option for both \"key column\" and \"value column\" inputs in <code>create-dictionary-from-datatable</code>. To import the mapping table I used the  <code>import-from-csv-file</code> as the mapping table, in this case, is a csv file. </p> <p>The resulting Newick file from <code>rename-newick-leaf-labels</code> was directed to a final adaptor, <code>export-text-to-file</code>. This allowed me to customise the final outfile, for instance, to set the output file name to be \"renamed_newick.nwk\" so it would have the correct file extension for most tree visualisation programs. </p> <p>Here is the final newick file. You will see that the \"SAL_xxx\" labels have been replaced, where possible, with the final SRA accession.  If there is no mapping provided, the tip label will be left unaltered. For instance, \"SAL_DB7870AA_AS\" does not have an SRA accession in the mapping file, and has been left alone in the final output.</p> <pre><code>((SRR1966467:0.00016804,((SRR7866992:9.3034e-06,SRR7457149:1.13873e-05)1:2.90208e-07,((SRR3286948:4.43003e-06,\n(SRR7192177:5.53755e-06,ERR3285039:6.64654e-07)1:8.8599e-07)1:1.77201e-06,(((ERR3285354:1.32901e-06,\n.....\n((SAL_DB7870AA_AS:2.65844e-06,SAL_DB7866AA_AS:1.55087e-06)1:2.2152e-07,SAL_DB7873AA_AS:4.20876e-06)1:4.4302e-07)1:1e-09)1:4.05628e-06)1:8.43982e-05)1:0.000219831,\n....\n</code></pre> <ol> <li> <p>Zhou Z, Alikhan NF, Mohamed K, Fan Y; Agama Study Group; Achtman M. The EnteroBase user's guide, with case studies on Salmonella transmissions, Yersinia pestis phylogeny, and Escherichia core genomic diversity. Genome Res. 2020 Jan;30(1):138-152. doi: 10.1101/gr.251678.119. Epub 2019 Dec 6. PMID: 31809257; PMCID: PMC6961584.\u00a0\u21a9</p> </li> </ol>"}]}